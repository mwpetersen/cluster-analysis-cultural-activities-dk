---
title: "A clustering analysis of cultural activities in a subset of Danish Municipalities"
subtitle: "Applied machine learning and big data, fall 2021"
author: "Mikkel Wittenburg Petersen"
date: "`r Sys.Date()`"
toc-title: Contents
output:
  pagedown::html_paged:
    toc: true
    fig_caption: true
    number_sections: true
    css:
      - "../styles/fonts.css"
      - default
      - "../styles/custom-style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(knitr)
library(tidyverse)
library(readxl)
library(readr)
library(clusterability)
library(factoextra)
library(NbClust)
library(mapDK)
library(janitor)
library(gt)
library(broom)

theme_set(theme_minimal())
```

```{r import-data}

df_cultural_activities <- read_excel("../data/use-of-cultural-activities-dk-2019.xlsx", skip = 2) %>%
  select(-1)

```

```{r wrangle data}

df_cultural_activities_clean <- df_cultural_activities %>%
  rename(municipality = `...2`) %>%
  filter(!str_detect(municipality, 'Region|Province')) %>%
  janitor::clean_names()

df_cultural_activities_all_numeric <- data.frame(df_cultural_activities_clean, row.names = 1) # Make the municipality column to an index

df_cultural_activities_scaled <- as.data.frame(scale(df_cultural_activities_all_numeric)) # Scale the data

```

# Introduction

This paper explores similarities and differences between Danish Municipalities in terms of cultural 
habits of their citizens. Which Municipalities are alike and which differ? And *how* are they similar or different? 

A common way to define different types of areas is to divide them into urban, suburban and
rural areas. Many analyses (see e.g. Pew Research Center, 2018) seek to explore whether there are systematic differences among these types of areas and communities — both politically, socially and culturally. In this paper I look at whether this division also emerges when looking at cultural
habits between citizens in primarily urban, suburban or rural Danish Municipalities.

The method I use to explore this is clustering. Specifically I use K-means and hierarchical
clustering to group Municipalities by the cultural habits of their citizens. Clustering 
is frequently used within the social sciences, especially in sociology (Ahlquist and Breunig, 2009).
Examples of the use of clustering within the social sciences are Chae (2010) who explore groups in political attitude, and Franzoni (2008) who explore groups of welfare regimes.  

The data is from a survey from 2019 by Statistics Denmark. In the survey they asked citizens in 44
Municipalities in Denmark about their use of cultural activities within the three months leading up to the survey. 

# Method
In this section I describe in detail how I explore similarities and differences between Danish Municipalities in terms of cultural habits of their citizens. Specifically, I describe the data and the methods used.

## The data
The data is from a Danish national survey from 2019 conducted by Statistics Denmark. Around 12.000
people aged 16 and up participated in the survey (Statistics Denmark, 2021:7). 

The purpose of the survey is to shed light on the cultural habits of the citizens of Denmark. Culture is in the survey defined in a broad sense and covers for instance concerts, theater, literature, cinema, exercise, computer games and other leisure activities. 

The participants in the survey were asked how often within the last three months leading up to the survey they had participated in for instance the cultural activities listed above. 

The data set used in this report is on an aggregated level. It shows for each municipality the percent of the adult population that had participated in the following cultural activities within the last three months leading up to the survey:

:::: {style="display: flex;"}

::: {}
- `r colnames(df_cultural_activities)[2]`
- `r colnames(df_cultural_activities)[3]`
- `r colnames(df_cultural_activities)[4]`
- `r colnames(df_cultural_activities)[5]`
:::

::: {}
- `r colnames(df_cultural_activities)[6]`
- `r colnames(df_cultural_activities)[7]`
- `r colnames(df_cultural_activities)[8]`
- `r colnames(df_cultural_activities)[9]`
- `r colnames(df_cultural_activities)[10]`
:::

::::

Here's a look at the first 3 rows and columns of the data set:
```{r}

gt(head(df_cultural_activities_clean[1:3], 3)) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = 2)
  ) %>%
  cols_label(
    municipality = "Municipality",
    have_been_to_the_cinema = "Been to cinema",
    have_been_to_concert = "Been to concert"
  )

```

## Clustering

Clustering refers to a variety of algorithms for discovering subgroups (i.e. clusters) in a data set (James et. al 2021: 497). The goal is therefore to partition observations in a data set into distinct groups, such that the observations in a given group are more similar to each other than observations in other groups (Hastie 2009: 501). As such, the goal is not to predict an outcome, as is the case in supervised learning. Clustering is often used during the exploratory phase of a data analysis. 

The similarity and dissimilarity between data points are determined by the distance between them. The larger the distance, the more dissimilar the data points are.  The measure of distance that is most popular is the Euclidean distance (Kabacoff 2021:425). The Euclidean distance between two observations (typically two rows in a data set) is calculated by taking the square root of the total sum of the squared pairwise differences between all variables in the data set. 

### Clustering algorithms used in this analysis
In this analysis I have chosen to include two different clustering algorithms: 
1. K-means clustering (belonging to the family of clustering approaches known as partitioning clustering) and 
2. Hierarchical agglomerative clustering. 

I have included both approaches to see how robust the results of the clustering are; do the two approaches cluster the data in the same way? 

#### K-means clustering
In K-means clustering you specify in advance the number of subgroups (K) the data should be partitioned into. When K is chosen, the algorithm consists of these steps (Kabacoff 2021:434):

- Randomly select K rows to serve as centroids
- Partition all observations to the closest centroid
- Calculate the average of all the observations within each cluster to get the new centroids
- Repartition each observations to the cluster with the nearest centroid
- Continue step 3 and 4 until the clusters stay the same.

The result of the above steps is a set of K clusters where the total sum of the within-cluster variation is as small as possible, given the data set and the specified number of clusters. The within-cluster variation in each cluster is calculated by taking the the sum of the squared Euclidean pairwise distances between all of the observations in the cluster, divided by the number of observations in the cluster (James et. al 2021:518).

#### Hierarchical clustering
Unlike K-means clustering, in hierarchical clustering you don’t choose the number of clusters in advance. Instead the number of clusters is chosen post hoc. The algorithm consists of these steps (Kabacoff 2021:428):

- Each observation starts as its own cluster
- Calculate the distances between all clusters
- The two clusters with the smallest distance between them are merged into one cluster
- Repeat steps 2 and 3 until all clusters are merged into one cluster with all the observations.

The result of these steps can be visualized in a so-called dendrogram.
As with K-means clustering, in hierarchical clustering the distance is often measured using the Euclidean measure of distance. Furthermore, when the clusters contain more than one observation, a measure of the distance between them needs to be chosen. This is called linkage. The most common types of linkage are complete, average, single and centroid (James et. al 2021:525). As an example, average linkage between two clusters is found by computing all pairwise distances between the observations in each cluster and taking the average of these distances.

#### Advantages and disadvantages of both algorithms
According to Kabacoff (2021:425), hierarchical clustering is most useful when you expect the clusters to 1) nested and 2) have a meaningful hierarchy. Another advantage is that you don’t have to specify the number of clusters in advance. A disadvantage of hierarchical clustering is that when an observation is assigned to a cluster, it will stay in that cluster and can’t be reassigned later. Furthermore, it can be difficult to interpret the result of a hierarchical cluster analysis if the data set is large (Kabacoff 2021:433). 

In such cases, K-means clustering may be preferred. Another advantage of K-means clustering is that observations can be partitioned into different clusters during the clustering process if that improves the overall solution. It is therefore a more flexible approach compared to hierarchical clustering. A disadvantage of K-means clustering is that it doesn’t perform well when the data is non-convex (Kabacoff 2021:434).

# Analysis

## K-means clustering

### Choosing number of clusters
The first step in the analysis is determining the optimal number of clusters. To do so I'll
use two methods: 1) the "Elbow" method, and 2) the "silhouette" method. Furthermore I'll look at a
summary of 26 methods to determine the optimal number of clusters. 

The idea behind K-means clustering is to minimize the total within-groups sums of squares, and at the same time partitioning the data set in a sensible number of clusters (if each data point had its own cluster the total within-groups sums of squares would be 0, but the cluster analysis would be pointless). We can plot the total within-groups sums of squares for different values of K, as shown below:

```{r fig1-cluster, fig.height=4, fig.cap="The total within-groups sums of squares for different values of K."}

fviz_nbclust(df_cultural_activities_scaled, 
             kmeans, 
             method = "wss",
             linecolor = "#2770D6") + 
  labs(title= "Optimal number of clusters: the elbow method") 

```
A strategy is to choose a number of clusters so that the total within-groups sums of squares doesn't change much by adding one more cluster. Thus in the plot you can look for a bend (the elbow) to determine the optimal number of clusters. Figure \@ref(fig:fig1-cluster) above shows "an elbow" at 2 or 3 clusters.

An alternative method is the "silhouette" method. The method measures the quality of a clustering  	(Boehmke, n.d.). The higher the average silhouette width, the better the clustering. The method computes the average width for every specified value of K. And the optimal number of clusters is the one with the highest average silhouette width.

```{r fig2-cluster, fig.height=4, fig.cap="The average silhouette width for different values of K."}

p <- fviz_nbclust(df_cultural_activities_scaled, 
             kmeans, 
             method = "silhouette",
             linecolor = "#2770D6"
             ) 

p + labs(title= "Optimal number of clusters: the silhouette method") 

```

In Figure \@ref(fig:fig2-cluster) above we can see that the silhouette method suggests K = 2 as the optimal number of clusters. And K = 3 is the second optimal number of clusters.

The `NbClust` package includes 30 methods for selecting the optimal value of K. How many of the 30 that can be used depends on the data set (Kabacoff 2021:436). Below is a summary of what these methods suggest as the optimal value of K for our data set at hand. 8 out of 26 methods/indices suggest 2 as the optimal value of K. 6 methods suggest K = 3.

```{r include=FALSE}

nc <- NbClust(df_cultural_activities_scaled,
              min.nc=2, 
              max.nc = 15, 
              method = "kmeans")

```

```{r fig3-cluster, fig.height=4, fig.cap="Number of methods in `NbClust` suggesting different values of K."}

p <- ggplot(data=as.data.frame(table(nc$Best.n[1,])), 
            aes(x=Var1, y=Freq)) +
  geom_col(fill = "#2770D6") +
  geom_text(
    aes(label = Freq),
    vjust = 2,
    size = 4, fontface = "bold", color = "white") +
  labs(title= "Optimal number of clusters: summary") +
  ylab("Frequency among all indices") + 
  xlab("Number of clusters K") +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.title=element_text(size=12),
    axis.text.y=element_blank(),
    axis.text.x = element_text(size = 14, 
                               margin=margin(t=-10)))
  

p

```

## Performing K-means clustering
A K-means clustering is performed with the `kmeans()` function. I have performed a clustering with both 2 and 3 clusters. The results can be seen below in the bivariate cluster plots, maps of Denmark and tables.

The bivariate cluster plots show the municipalities on the first two principal components. The principal components summarize the variation in the 9 variables about cultural habits.

```{r k-means-clustering}
set.seed(1234)

df_cultural_activities_clust_2 <- kmeans(df_cultural_activities_scaled, centers = 2, nstart = 50)

df_cultural_activities_clust_3 <- kmeans(df_cultural_activities_scaled, centers = 3, nstart = 50)


```

```{r}

df_cluster_2 <- augment(df_cultural_activities_clust_2, df_cultural_activities_clean) %>%
  rename(Cluster = `.cluster`)

df_cluster_3 <- augment(df_cultural_activities_clust_3, df_cultural_activities_clean) %>%
  rename(Cluster = `.cluster`)

```

```{r fig4-cluster, fig.height=4, fig.cap="Bivariate cluster plot with K = 2"}

fviz_cluster(df_cultural_activities_clust_2, 
             data = df_cultural_activities_scaled,
             ggtheme = theme_minimal()) + 
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(title= "Bivariate cluster plot with K = 2") 

```

In plot \@ref(fig:fig5-cluster) below we can see that the main difference between running the analysis with 2 or 3 clusters is that cluster 2 in plot \@ref(fig:fig4-cluster) is now divided in two. Frederiksberg, Copenhagen, Gentofte and Aarhus are now in their own cluster. 
```{r fig5-cluster, fig.height=4, fig.cap="Bivariate cluster plot with K = 3"}

fviz_cluster(df_cultural_activities_clust_3, 
             data = df_cultural_activities_scaled,
             ggtheme = theme_minimal()) + 
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(title= "Bivariate cluster plot with K = 3") 

```

Another way to visualize the clusters are with maps. I have done this below. What do the maps show? Do they show that there are systematic differences in the cultural habits between citizens in primarily urban, suburban or rural Danish Municipalities? To some extent they do. But the picture is not that clear cut.

```{r map-clusters-2, warning=FALSE, message=FALSE}

df_cluster_2$municipality[df_cluster_2$municipality== "Copenhagen"] <- "København"

mapDK(values = "Cluster", id = "municipality", data = df_cluster_2)

```

```{r map-clusters-3, warning=FALSE, message=FALSE}

df_cluster_3$municipality[df_cluster_3$municipality== "Copenhagen"] <- "København"

mapDK(values = "Cluster", id = "municipality", data = df_cluster_3)

```

## How do the clusters differ?

```{r}

df_cluster_2_summary <- df_cluster_2 %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean)) %>%
  mutate(across(where(is.numeric), round, 1))

df_cluster_3_summary <- df_cluster_3 %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean)) %>%
  mutate(across(where(is.numeric), round, 1))

```


```{r}

df_cluster_2_summary %>%
  select(1:4) %>%
  gt(.) %>%
    tab_options(table.align='left',
                table.width = pct(100),
                column_labels.border.bottom.width= px(3),
                column_labels.border.bottom.color= "black",
                column_labels.border.top.color = "white") %>%
    tab_style(
      style = list(
        cell_text(weight = "bold")
        ),
      locations = cells_column_labels()) %>%
    tab_style(
      style = list(
        cell_fill(color = "white")
      ),
      locations = cells_body(
        rows = 2)
    ) %>%
  cols_label(
    have_been_to_the_cinema = "Been to the cinema",
    have_been_to_concert = "Been to concert",
    have_seen_visual_arts_on_pupose = "Seen visual arts"
  )


```

test

```{r}

df_cluster_2_summary %>%
  select(c(1, 5, 6, 7)) %>%
  gt(.) %>%
    tab_options(table.align='left',
                table.width = pct(100),
                column_labels.border.bottom.width= px(3),
                column_labels.border.bottom.color= "black",
                column_labels.border.top.color = "white") %>%
    tab_style(
      style = list(
        cell_text(weight = "bold")
        ),
      locations = cells_column_labels()) %>%
    tab_style(
      style = list(
        cell_fill(color = "white")
      ),
      locations = cells_body(
        rows = 2)
    ) %>%
  cols_label(
    have_read_or_listened_to_fiction = "Read or listened to fiction",
    have_read_or_listened_to_non_fiction = "Read or listened to non-fiction",
    have_visited_the_library_physical_visit = "Visited library"
  )

```

test

```{r}

df_cluster_2_summary %>%
  select(c(1, 8, 9, 10)) %>%
  gt(.) %>%
    tab_options(table.align='left',
                table.width = pct(100),
                column_labels.border.bottom.width= px(3),
                column_labels.border.bottom.color= "black",
                column_labels.border.top.color = "white") %>%
    tab_style(
      style = list(
        cell_text(weight = "bold")
        ),
      locations = cells_column_labels()) %>%
    tab_style(
      style = list(
        cell_fill(color = "white")
      ),
      locations = cells_body(
        rows = 2)
    ) %>%
  cols_label(
    have_used_the_librarys_digital_services = "Used librarys digital services",
    visited_a_museum_etc = "Visited museum",
    have_watched_performing_arts_in_theater_opera_festivals_or_in_public_spaces = "Watched performing arts"
  )

```

## Validation of the results

### Are there clusters in the data?
```{r dip-test}

clusterabilitytest(df_cultural_activities_scaled, "dip")

```


# Discussion

# Literature

- Ahlquist, J. S. and Breunig, C. (2009): "Country clustering in comparative po-
litical economy". MPIfG Discussion Paper 09/5, Max-Planck-Institut für
Gesellschaftsforschung, Köln.

- Boehmke, Bradley (n.d.). K-means Cluster Analysis. Retrieved December 6, 2021, from http://uc-r.github.io/kmeans_clustering

- Chae, H. (2010): "South Korean attitudes toward the ROK−U.S. alliance: group
analysis". PS: Political Science and Politics 43, 493-501.

- Franzoni, J. M. (2008): "Welfare regimes in Latin America: capturing constella-
tions of markets, families, and policies". Latin American Politics and Society
50, 67-100.

- James, Gareth, et al. An Introduction to Statistical Learning. 2nd ed., Springer, 2021.

- Kabacoff, R. I. (2021). R in Action (3rd ed.). Manning Publications.

- Pew Research Center (2018): “What Unites and Divides Urban, Suburban and Rural Communities”

- Statistics Denmark (2021): "Statistikdokumentation for Kulturvaneundersøgelsen 2021"
